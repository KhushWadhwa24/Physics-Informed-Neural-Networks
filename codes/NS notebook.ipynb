{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viscous Flow past a Circular Cylinder\n",
    "This notebook aims to solve the Navier Stokes equations past a circular cylinder. We use some data from a lower order potential flow solution for flow past a circular cylinder and see whether a PINN can predict the boundary layer and separation in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-infromed Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        # Initialize hidden layers and output layer with custom weight initialization\n",
    "        self.hidden_layers = [tf.keras.layers.Dense(layer, activation='tanh') for layer in layers[:-1]]\n",
    "        self.output_layer = tf.keras.layers.Dense(layers[-1])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Manually build each layer to ensure proper initialization\n",
    "        for layer in self.hidden_layers:\n",
    "            layer.build(input_shape)\n",
    "            input_shape = (input_shape[0], layer.units)\n",
    "        self.output_layer.build(input_shape)\n",
    "        super(PINN, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass through the network\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring an instance of the PINN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [2, 70, 50, 30, 10, 3]   \n",
    "# Outputs: U, V, P, \n",
    "\n",
    "model = PINN(layers)\n",
    "# declare the model\n",
    "\n",
    "#hyperparameters\n",
    "#learning_rate (float or schedule, default=0.001): The step size for updating weights during optimization.\n",
    "#beta_1 (float, default=0.9): Exponential decay rate for the first moment estimates (mean of gradients).\n",
    "#beta_2 (float, default=0.999): Exponential decay rate for the second moment estimates (variance of gradients).\n",
    "#amsgrad (boolean, default=False):Whether to apply the AMSGrad variant of the optimizer.\n",
    "lr = 0.001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr,beta_1=b1,beta_2=b2,amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pinn_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"pinn_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,550</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,530</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">310</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │           \u001b[38;5;34m210\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m3,550\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m1,530\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m310\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,639</span> (22.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,639\u001b[0m (22.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,639</span> (22.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,639\u001b[0m (22.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.build(input_shape=(None, 2)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Governing Equations and Loss\n",
    "The continuity eqaution\n",
    "$$\n",
    "\\frac{\\partial U}{\\partial x} + \\frac{\\partial V}{\\partial y} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuity_loss(model, x, y):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([x, y])\n",
    "        inputs = tf.stack([x, y], axis=1)[:,:,0]\n",
    "        predictions = model(inputs)\n",
    "        U = predictions[:, 0]  \n",
    "        V = predictions[:, 1]  \n",
    "    dU_dx = tape.gradient(U, x)\n",
    "    dV_dy = tape.gradient(V, y)\n",
    "    \n",
    "    continuity_residual = dU_dx + dV_dy\n",
    "    continuity_loss = tf.reduce_mean(tf.square(continuity_residual))\n",
    "\n",
    "    return continuity_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X-momentum equation\n",
    "$$\n",
    "U \\frac{\\partial U}{\\partial x} + V \\frac{\\partial U}{\\partial y} = -\\frac{\\partial P}{\\partial x} \n",
    "+ \\frac{1}{\\mathrm{Re}} \\left( \\frac{\\partial^2 U}{\\partial x^2} + \\frac{\\partial^2 U}{\\partial y^2} \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_momentum_loss(model, x, y, Re):\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([x, y])\n",
    "            tape2.watch([x, y])\n",
    "            inputs = tf.stack([x, y], axis=1)[:,:,0]\n",
    "            predictions = model(inputs)\n",
    "            U = predictions[:, 0]  # U: x-velocity\n",
    "            V = predictions[:, 1]  # V: y-velocity\n",
    "            P = predictions[:, 2]  # P: pressure\n",
    "        \n",
    "        # First-order derivatives\n",
    "        dU_dx = tape1.gradient(U, x)\n",
    "        dU_dy = tape1.gradient(U, y)\n",
    "        dP_dx = tape1.gradient(P, x)\n",
    "        \n",
    "        # Second-order derivatives\n",
    "        d2U_dx2 = tape2.gradient(dU_dx, x)\n",
    "        d2U_dy2 = tape2.gradient(dU_dy, y)\n",
    "\n",
    "    # X-momentum equation residual\n",
    "    x_momentum_residual = U*dU_dx + V*dU_dy + dP_dx -(1/Re)*(d2U_dx2+d2U_dy2)\n",
    "    x_momentum_loss = tf.reduce_mean(tf.square(x_momentum_residual))\n",
    "\n",
    "    return x_momentum_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Y-momentum equation\n",
    "$$\n",
    "U \\frac{\\partial V}{\\partial x} + V \\frac{\\partial V}{\\partial y} = \n",
    "-\\frac{\\partial P}{\\partial y} \n",
    "+ \\frac{1}{\\mathrm{Re}} \\left( \\frac{\\partial^2 V}{\\partial x^2} + \\frac{\\partial^2 V}{\\partial y^2} \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_momentum_loss(model, x, y, Re):\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([x, y])\n",
    "            tape2.watch([x, y])\n",
    "            inputs = tf.stack([x, y], axis=1)[:,:,0]\n",
    "            predictions = model(inputs)\n",
    "            U = predictions[:, 0]  # U: x-velocity\n",
    "            V = predictions[:, 1]  # V: y-velocity\n",
    "            P = predictions[:, 2]  # P: pressure\n",
    "\n",
    "        \n",
    "        # First-order derivatives\n",
    "        dV_dx = tape1.gradient(V, x)\n",
    "        dV_dy = tape1.gradient(V, y)\n",
    "        dP_dy = tape1.gradient(P, y)\n",
    "\n",
    "        \n",
    "        # Second-order derivatives\n",
    "        d2V_dx2 = tape2.gradient(dV_dx, x)\n",
    "        d2V_dy2 = tape2.gradient(dV_dy, y)\n",
    "\n",
    "    # X-momentum equation residual\n",
    "    y_momentum_residual = U*dV_dx + V*dV_dy + dP_dy -(1/Re)*(d2V_dx2+d2V_dy2)\n",
    "    y_momentum_loss = tf.reduce_mean(tf.square(y_momentum_residual))\n",
    "\n",
    "    return y_momentum_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No-slip \\& No-penetration at the cylindrical boundary\n",
    "$$\n",
    "U = V = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wall_boundary_loss(model, x_wall, y_wall):\n",
    "    inputs = tf.stack([x_wall, y_wall], axis=1)[:,:,0]\n",
    "    predictions = model(inputs)\n",
    "    U = predictions[:, 0]  # U: x-velocity\n",
    "    V = predictions[:, 1]  # V: y-velocity\n",
    "\n",
    "\n",
    "    velocity_residual = tf.square(U) + tf.square(V)\n",
    "    # Combine losses\n",
    "    wall_loss = tf.reduce_mean(velocity_residual)\n",
    "    return wall_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inflow \n",
    "$$\n",
    "U = 1\n",
    "$$\n",
    "$$\n",
    "V = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflow_boundary_loss(model, x_in, y_in):\n",
    "\n",
    "    inputs = tf.stack([x_in, y_in], axis=1)[:,:,0]\n",
    "    predictions = model(inputs)\n",
    "    U = predictions[:, 0]  # U: x-velocity\n",
    "    V = predictions[:, 1]  # V: y-velocity\n",
    "\n",
    "    # Residuals for boundary conditions\n",
    "    velocity_residual = tf.square(U - 1) + tf.square(V)  \n",
    "\n",
    "\n",
    "    # Combine losses\n",
    "    inflow_loss = tf.reduce_mean(velocity_residual)\n",
    "\n",
    "    return inflow_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper and lower free boundaries\n",
    "\n",
    "$$\n",
    "\\frac{\\partial U}{\\partial y} = 0\n",
    "$$\n",
    "$$\n",
    "V = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_lower_boundary_loss(model, x_combined, y_combined):\n",
    "\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([x_combined, y_combined])\n",
    "        # Stack inputs for the model\n",
    "        inputs = tf.stack([x_combined, y_combined], axis=1)[:,:,0]\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # Extract predicted values\n",
    "        U  = predictions[:, 0]  # U: x-velocity\n",
    "        V  = predictions[:, 1]  # V: y-velocity\n",
    "\n",
    "\n",
    "    # Compute derivatives with respect to y\n",
    "    dU_dy = tape.gradient(U, y_combined)\n",
    "\n",
    "\n",
    "    # Residuals for boundary conditions\n",
    "    velocity_residual = tf.square(V) \n",
    "    derivative_residual = tf.square(dU_dy) \n",
    "\n",
    "    # Combine losses\n",
    "    upper_lower_loss = tf.reduce_mean(velocity_residual + derivative_residual)\n",
    "    return upper_lower_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outflow \n",
    "$$\n",
    "\\frac{2}{Re}\\frac{\\partial U}{\\partial x} - p = 0\n",
    "$$\n",
    "$$\n",
    "\\frac{2}{Re}\\frac{\\partial V}{\\partial y} - p = 0\n",
    "$$\n",
    "$$\n",
    " \\frac{\\partial U}{\\partial y} + \\frac{\\partial V}{\\partial x} =  0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outflow_loss(model, x_out, y_out,Re):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([x_out, y_out])\n",
    "        # Stack inputs for the model\n",
    "        inputs = tf.stack([x_out, y_out], axis=1)[:,:,0]\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # Extract predicted values\n",
    "        U = predictions[:, 0]  # U: x-velocity\n",
    "        V = predictions[:, 1]  # V: y-velocity\n",
    "        P = predictions[:, 2]  # P: pressure\n",
    "\n",
    "\n",
    "    # Compute derivatives with respect to x\n",
    "    dU_dx = tape.gradient(U, x_out)\n",
    "    dV_dx = tape.gradient(V, x_out)\n",
    "    dU_dy = tape.gradient(U, y_out)\n",
    "    dV_dy = tape.gradient(V, y_out)\n",
    "\n",
    "\n",
    "    # Residuals for zero-gradient conditions\n",
    "    residual =   tf.square((2/Re)*dU_dx - P) + tf.square((2/Re)*dV_dy - P) + tf.square(dU_dy+ dV_dx)\n",
    "    \n",
    "    # Combine losses\n",
    "    outflow_loss = tf.reduce_mean(residual)\n",
    "\n",
    "    return outflow_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loss: Using Potential Flow Solution for Training the RANS PINN\n",
    "\n",
    "Earlier, a PINN model has been trained to accurately predict the inviscid, icompressible flow past a cylinder. The idea is to use that model for training data in the upstream of the cylinder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"potential flow solution.png\" alt=\"Description of image\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only the part upstream of the cylinder and away from the walls will be used since this is the region where the invisicid assumptions are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Training_points_1.jpg\" alt=\"Description of image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data_loss.X\n",
    "data_y = data_loss.Y\n",
    "data_U = data_loss.U\n",
    "data_V = data_loss.V\n",
    "data_P = data_loss.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pot_loss(model,x,y,u,v,p):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([x, y])\n",
    "        # Stack inputs for the model\n",
    "        inputs = tf.stack([x, y], axis=1)[:,:,0]\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # Extract predicted values\n",
    "        U = predictions[:, 0]  # U: x-velocity\n",
    "        V = predictions[:, 1]  # V: y-velocity\n",
    "        P = predictions[:, 2]\n",
    "    loss = tf.square(U-u) + tf.square(V-v) + tf.square(P-p)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "xint = Points.xinterior\n",
    "yint = Points.yinterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "xwall = Points.x_wall\n",
    "ywall = Points.y_wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "xin = Points.xinflow\n",
    "yin = Points.yinflow\n",
    "xout = Points.xoutflow\n",
    "yout = Points.youtflow\n",
    "xfar = Points.xfar\n",
    "yfar = Points.yfar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "totalloss = []\n",
    "\n",
    "inflow = []\n",
    "outflow = []\n",
    "farflow = []\n",
    "wallloss = []\n",
    "\n",
    "cont = []\n",
    "xmom = []\n",
    "ymom = []\n",
    "\n",
    "dataloss = []\n",
    "epochnum = []\n",
    "Re = 10\n",
    "\n",
    "Ny = 1000\n",
    "Nx = 2000\n",
    "xgrid = np.linspace(-Points.a,Points.b,Nx)\n",
    "ygrid = np.linspace(-Points.a,Points.a,Ny)\n",
    "xmesh,ymesh = np.meshgrid(xgrid,ygrid)\n",
    "x_flat = tf.convert_to_tensor(xmesh.flatten().reshape(-1, 1), dtype=tf.float32)\n",
    "y_flat = tf.convert_to_tensor(ymesh.flatten().reshape(-1, 1), dtype=tf.float32)\n",
    "xy_tensor = tf.concat([x_flat, y_flat], axis=1)\n",
    "mask = (xmesh**2 + ymesh**2 < 1)\n",
    "predictions = model(xy_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  0 , loss =  22.771713\n",
      "n =  1 , loss =  16.612137\n",
      "n =  2 , loss =  11.899753\n",
      "n =  3 , loss =  8.553914\n",
      "n =  4 , loss =  6.377908\n",
      "n =  5 , loss =  5.101742\n",
      "n =  6 , loss =  4.449416\n",
      "n =  7 , loss =  4.187975\n",
      "n =  8 , loss =  4.1459017\n",
      "n =  9 , loss =  4.206899\n",
      "n =  10 , loss =  4.294173\n",
      "n =  11 , loss =  4.3572884\n",
      "n =  12 , loss =  4.3651643\n",
      "n =  13 , loss =  4.3041725\n",
      "n =  14 , loss =  4.177534\n",
      "n =  15 , loss =  4.001445\n",
      "n =  16 , loss =  3.798286\n",
      "n =  17 , loss =  3.589999\n",
      "n =  18 , loss =  3.3934798\n",
      "n =  19 , loss =  3.2186298\n",
      "n =  20 , loss =  3.0688496\n",
      "n =  21 , loss =  2.9428608\n",
      "n =  22 , loss =  2.8365436\n",
      "n =  23 , loss =  2.7441378\n",
      "n =  24 , loss =  2.659157\n",
      "n =  25 , loss =  2.5755863\n",
      "n =  26 , loss =  2.4891915\n",
      "n =  27 , loss =  2.398103\n",
      "n =  28 , loss =  2.3024132\n",
      "n =  29 , loss =  2.2034128\n",
      "n =  30 , loss =  2.1031601\n",
      "n =  31 , loss =  2.0044951\n",
      "n =  32 , loss =  1.9110808\n",
      "n =  33 , loss =  1.826977\n",
      "n =  34 , loss =  1.7556255\n",
      "n =  35 , loss =  1.6986765\n",
      "n =  36 , loss =  1.6553477\n",
      "n =  37 , loss =  1.6227362\n",
      "n =  38 , loss =  1.5968928\n",
      "n =  39 , loss =  1.5740348\n",
      "n =  40 , loss =  1.5513169\n",
      "n =  41 , loss =  1.5270133\n",
      "n =  42 , loss =  1.5003434\n",
      "n =  43 , loss =  1.4712228\n",
      "n =  44 , loss =  1.4400525\n",
      "n =  45 , loss =  1.4075345\n",
      "n =  46 , loss =  1.374514\n",
      "n =  47 , loss =  1.3418841\n",
      "n =  48 , loss =  1.3105576\n",
      "n =  49 , loss =  1.2814287\n",
      "n =  50 , loss =  1.2552491\n",
      "n =  51 , loss =  1.2324337\n",
      "n =  52 , loss =  1.2129186\n",
      "n =  53 , loss =  1.1961987\n",
      "n =  54 , loss =  1.1815352\n",
      "n =  55 , loss =  1.1682087\n",
      "n =  56 , loss =  1.1556766\n",
      "n =  57 , loss =  1.1435976\n",
      "n =  58 , loss =  1.1317925\n",
      "n =  59 , loss =  1.1202087\n",
      "n =  60 , loss =  1.1089106\n",
      "n =  61 , loss =  1.0980558\n",
      "n =  62 , loss =  1.0878432\n",
      "n =  63 , loss =  1.0784441\n",
      "n =  64 , loss =  1.0699484\n",
      "n =  65 , loss =  1.062338\n",
      "n =  66 , loss =  1.0554814\n",
      "n =  67 , loss =  1.0491588\n",
      "n =  68 , loss =  1.0431192\n",
      "n =  69 , loss =  1.0371627\n",
      "n =  70 , loss =  1.0312085\n",
      "n =  71 , loss =  1.0253046\n",
      "n =  72 , loss =  1.0195769\n",
      "n =  73 , loss =  1.0141554\n",
      "n =  74 , loss =  1.0091231\n",
      "n =  75 , loss =  1.0045078\n",
      "n =  76 , loss =  1.000297\n",
      "n =  77 , loss =  0.9964534\n",
      "n =  78 , loss =  0.99292403\n",
      "n =  79 , loss =  0.98964417\n",
      "n =  80 , loss =  0.98654747\n",
      "n =  81 , loss =  0.9835754\n",
      "n =  82 , loss =  0.9806873\n",
      "n =  83 , loss =  0.9778661\n",
      "n =  84 , loss =  0.975118\n",
      "n =  85 , loss =  0.9724655\n",
      "n =  86 , loss =  0.9699328\n",
      "n =  87 , loss =  0.9675306\n",
      "n =  88 , loss =  0.965253\n",
      "n =  89 , loss =  0.9630858\n",
      "n =  90 , loss =  0.96101654\n",
      "n =  91 , loss =  0.95903957\n",
      "n =  92 , loss =  0.9571535\n",
      "n =  93 , loss =  0.9553566\n",
      "n =  94 , loss =  0.9536469\n",
      "n =  95 , loss =  0.9520243\n",
      "n =  96 , loss =  0.95049053\n",
      "n =  97 , loss =  0.9490463\n",
      "n =  98 , loss =  0.9476868\n",
      "n =  99 , loss =  0.9464006\n",
      "n =  100 , loss =  0.9451713\n",
      "n =  101 , loss =  0.9439825\n",
      "n =  102 , loss =  0.9428234\n",
      "n =  103 , loss =  0.9416916\n",
      "n =  104 , loss =  0.9405925\n",
      "n =  105 , loss =  0.939536\n",
      "n =  106 , loss =  0.9385296\n",
      "n =  107 , loss =  0.93757683\n",
      "n =  108 , loss =  0.93667686\n",
      "n =  109 , loss =  0.9358259\n",
      "n =  110 , loss =  0.9350182\n",
      "n =  111 , loss =  0.93424726\n",
      "n =  112 , loss =  0.93350667\n",
      "n =  113 , loss =  0.9327915\n",
      "n =  114 , loss =  0.9320999\n",
      "n =  115 , loss =  0.9314319\n",
      "n =  116 , loss =  0.93078834\n",
      "n =  117 , loss =  0.93016976\n",
      "n =  118 , loss =  0.92957526\n",
      "n =  119 , loss =  0.92900324\n",
      "n =  120 , loss =  0.9284518\n",
      "n =  121 , loss =  0.92791957\n",
      "n =  122 , loss =  0.9274059\n",
      "n =  123 , loss =  0.9269107\n",
      "n =  124 , loss =  0.92643344\n",
      "n =  125 , loss =  0.9259739\n",
      "n =  126 , loss =  0.9255309\n",
      "n =  127 , loss =  0.9251036\n",
      "n =  128 , loss =  0.924691\n",
      "n =  129 , loss =  0.92429143\n",
      "n =  130 , loss =  0.9239039\n",
      "n =  131 , loss =  0.92352706\n",
      "n =  132 , loss =  0.92316055\n",
      "n =  133 , loss =  0.922804\n",
      "n =  134 , loss =  0.92245764\n",
      "n =  135 , loss =  0.9221214\n",
      "n =  136 , loss =  0.92179507\n",
      "n =  137 , loss =  0.92147833\n",
      "n =  138 , loss =  0.92117053\n",
      "n =  139 , loss =  0.92087096\n",
      "n =  140 , loss =  0.92057896\n",
      "n =  141 , loss =  0.92029417\n",
      "n =  142 , loss =  0.92001617\n",
      "n =  143 , loss =  0.9197443\n",
      "n =  144 , loss =  0.9194788\n",
      "n =  145 , loss =  0.9192195\n",
      "n =  146 , loss =  0.918966\n",
      "n =  147 , loss =  0.9187181\n",
      "n =  148 , loss =  0.91847557\n",
      "n =  149 , loss =  0.91823786\n",
      "n =  150 , loss =  0.91800493\n",
      "n =  151 , loss =  0.9177765\n",
      "n =  152 , loss =  0.9175527\n",
      "n =  153 , loss =  0.9173331\n",
      "n =  154 , loss =  0.9171176\n",
      "n =  155 , loss =  0.91690624\n",
      "n =  156 , loss =  0.91669846\n",
      "n =  157 , loss =  0.91649437\n",
      "n =  158 , loss =  0.9162937\n",
      "n =  159 , loss =  0.91609645\n",
      "n =  160 , loss =  0.91590214\n",
      "n =  161 , loss =  0.91571105\n",
      "n =  162 , loss =  0.9155229\n",
      "n =  163 , loss =  0.9153377\n",
      "n =  164 , loss =  0.9151553\n",
      "n =  165 , loss =  0.9149756\n",
      "n =  166 , loss =  0.91479856\n",
      "n =  167 , loss =  0.91462386\n",
      "n =  168 , loss =  0.9144516\n",
      "n =  169 , loss =  0.9142817\n",
      "n =  170 , loss =  0.9141141\n",
      "n =  171 , loss =  0.9139487\n",
      "n =  172 , loss =  0.91378546\n",
      "n =  173 , loss =  0.9136243\n",
      "n =  174 , loss =  0.91346514\n",
      "n =  175 , loss =  0.9133079\n",
      "n =  176 , loss =  0.9131525\n",
      "n =  177 , loss =  0.9129991\n",
      "n =  178 , loss =  0.9128474\n",
      "n =  179 , loss =  0.91269743\n",
      "n =  180 , loss =  0.9125492\n",
      "n =  181 , loss =  0.9124025\n",
      "n =  182 , loss =  0.9122577\n",
      "n =  183 , loss =  0.91211426\n",
      "n =  184 , loss =  0.9119725\n",
      "n =  185 , loss =  0.91183215\n",
      "n =  186 , loss =  0.91169333\n",
      "n =  187 , loss =  0.91155595\n",
      "n =  188 , loss =  0.91142005\n",
      "n =  189 , loss =  0.91128546\n",
      "n =  190 , loss =  0.91115224\n",
      "n =  191 , loss =  0.9110204\n",
      "n =  192 , loss =  0.9108898\n",
      "n =  193 , loss =  0.9107604\n",
      "n =  194 , loss =  0.91063243\n",
      "n =  195 , loss =  0.9105057\n",
      "n =  196 , loss =  0.9103802\n",
      "n =  197 , loss =  0.9102557\n",
      "n =  198 , loss =  0.91013235\n",
      "n =  199 , loss =  0.91001034\n",
      "n =  200 , loss =  0.9098894\n",
      "n =  201 , loss =  0.9097696\n",
      "n =  202 , loss =  0.90965074\n",
      "n =  203 , loss =  0.90953314\n",
      "n =  204 , loss =  0.90941644\n",
      "n =  205 , loss =  0.9093008\n",
      "n =  206 , loss =  0.9091863\n",
      "n =  207 , loss =  0.90907276\n",
      "n =  208 , loss =  0.90896034\n",
      "n =  209 , loss =  0.9088488\n",
      "n =  210 , loss =  0.9087381\n",
      "n =  211 , loss =  0.9086285\n",
      "n =  212 , loss =  0.9085198\n",
      "n =  213 , loss =  0.908412\n",
      "n =  214 , loss =  0.9083052\n",
      "n =  215 , loss =  0.90819925\n",
      "n =  216 , loss =  0.9080942\n",
      "n =  217 , loss =  0.9079901\n",
      "n =  218 , loss =  0.9078868\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        if(epoch<1000):\n",
    "            pdeweight = 1\n",
    "            bcweight1 = 10\n",
    "            bcweight2 = 0.1\n",
    "            data_weight = 10\n",
    "     \n",
    "            \n",
    "        epochnum.append(epoch)\n",
    "        cont.append(    pdeweight*tf.cast(continuity_loss(model,xint,yint   ),dtype = tf.float32))\n",
    "        xmom.append(    pdeweight*tf.cast(x_momentum_loss(model,xint,yint,Re),dtype = tf.float32))\n",
    "        ymom.append(    pdeweight*tf.cast(y_momentum_loss(model,xint,yint,Re),dtype = tf.float32))\n",
    "        \n",
    "        inflow.append(  bcweight1*tf.cast(inflow_boundary_loss(model,xin,yin),dtype = tf.float32))\n",
    "        outflow.append( bcweight1*tf.cast(outflow_loss(       model,xout,yout,Re             ),dtype = tf.float32))\n",
    "        farflow.append( bcweight2*tf.cast(upper_lower_boundary_loss(model,xfar,yfar),dtype = tf.float32))\n",
    "        wallloss.append(bcweight2*tf.cast(wall_boundary_loss(model,xwall,ywall),dtype = tf.float32))\n",
    "        dataloss.append(data_weight*tf.cast(data_pot_loss(model,data_x,data_y,data_U,data_V,data_P),dtype = tf.float32))\n",
    "            \n",
    "        \n",
    "        loss = ((cont[-1] + xmom[-1] + ymom[-1]) + (inflow[-1] + outflow[-1] + farflow[-1] + wallloss[-1]) + dataloss[-1])#/#(3*pdeweight+2*bcweight2+2*bcweight1+data_weight)\n",
    "\n",
    "        totalloss.append(loss)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        print(\"n = \", epoch,\", loss = \",loss.numpy())\n",
    "        if(epoch% 20==0):\n",
    "            #print(\"n = \", epoch,\", loss = \",loss.numpy())  \n",
    "            #clear_output(wait=True)\n",
    "            \n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.semilogy(totalloss,label = \"total loss\")\n",
    "            plt.semilogy(cont,label = \"continuity\")\n",
    "            plt.semilogy(xmom,label = \"x momentum\")\n",
    "            plt.semilogy(ymom,label = \"y momentum\")\n",
    "            plt.semilogy(inflow,label = \"inflow bc\")\n",
    "            plt.semilogy(outflow,label = \"outflow bc\")\n",
    "            plt.semilogy(farflow,label = \"farfield bc\")\n",
    "            plt.semilogy(wallloss,label = \"wall bc\")\n",
    "            plt.semilogy(dataloss,label = \"Potential Flow\")\n",
    "            plt.legend()\n",
    "            plt.title(\"MSE (with data)\")\n",
    "            plt.grid()\n",
    "            plt.savefig(\"MSE with data.jpg\")\n",
    "            plt.close()\n",
    "        if (epoch%100 == 0):\n",
    "           # model.export('./Model')\n",
    "            predictions = model(xy_tensor)\n",
    "            \n",
    "            # Extract individual components from predictions\n",
    "\n",
    "            U  = predictions[:, 0].numpy().reshape(xmesh.shape)\n",
    "            V  = predictions[:, 1].numpy().reshape(xmesh.shape)\n",
    "            P  = predictions[:, 2].numpy().reshape(xmesh.shape)\n",
    "\n",
    "            U[mask]  = np.nan\n",
    "            V[mask]  = np.nan\n",
    "            P[mask]  = np.nan\n",
    "            fields = {'U': U, 'V': V, 'P': P, }\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            for i, (field_name, field_data) in enumerate(fields.items(), 1):\n",
    "                plt.subplot(2, 3, i)\n",
    "                contour = plt.contourf(xmesh, ymesh, field_data, levels=50, cmap='viridis')\n",
    "                plt.colorbar(contour, label=field_name)\n",
    "                plt.title(f\"{field_name} Contour\")\n",
    "                plt.xlabel(\"x\")\n",
    "                plt.ylabel(\"y\")\n",
    "                plt.gca().set_aspect('equal')\n",
    "\n",
    "\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"Plot\" +str(epoch) +\".png\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting results\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, (field_name, field_data) in enumerate(fields.items(), 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    contour = plt.contourf(xmesh, ymesh, field_data, levels=50, cmap='viridis')\n",
    "    plt.colorbar(contour, label=field_name)\n",
    "    plt.title(f\"{field_name} Contour\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.gca().set_aspect('equal')\n",
    "    \n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plot.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
